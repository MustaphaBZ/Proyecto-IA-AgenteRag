{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MustaphaBZ/Proyecto-IA-AgenteRag/blob/main/Proyecto_Final_AgenteRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROYECTO AGENTE RAG"
      ],
      "metadata": {
        "id": "hGAzOpOgGP3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conectar con Drive"
      ],
      "metadata": {
        "id": "7eXP0-rrJrS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n1O9jwa1KYpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip"
      ],
      "metadata": {
        "id": "OTQPziiyJ2Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instalar dependencias necesarias\n",
        "!pip install pinecone\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install bitsandbytes\n",
        "!pip install langchain-community\n",
        "!pip install torch\n",
        "!pip install pdfplumber\n",
        "!pip install langgraph==0.3.1\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "b1KFWKs9KZUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1: Importaciones"
      ],
      "metadata": {
        "id": "uWMxjLWgGUcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS ===\n",
        "from typing import Optional, List\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.tools import StructuredTool\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "import torch\n",
        "import os\n",
        "import unicodedata\n",
        "import requests\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "2ACUBqoEH4Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2: Tipos y clases"
      ],
      "metadata": {
        "id": "dpe4VoJqGbVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === TIPOS ===\n",
        "class WorkflowState(TypedDict, total=False):\n",
        "    directorio: str\n",
        "    carpeta: str\n",
        "    archivos: Optional[str]\n",
        "    archivos_txt: Optional[str]\n",
        "    conversion: Optional[str]\n",
        "    indexacion: Optional[str]\n",
        "    documentos: Optional[List[LangchainDocument]]\n",
        "    pregunta: Optional[str]\n",
        "    respuesta: Optional[str]\n",
        "    mensaje_final: Optional[str]\n",
        "\n",
        "class DirectorioInput(BaseModel):\n",
        "    directorio: str\n",
        "\n",
        "class DirectorioCarpetaInput(BaseModel):\n",
        "    directorio: str\n",
        "    carpeta: str\n",
        "\n",
        "class CarpetaInput(BaseModel):\n",
        "    carpeta: str\n",
        "\n",
        "class PreguntaInput(BaseModel):\n",
        "    pregunta: str\n",
        "\n",
        "class PreguntaConDocsInput(BaseModel):\n",
        "    pregunta: str\n",
        "    documentos: List[LangchainDocument]\n"
      ],
      "metadata": {
        "id": "Fl7idQ2KH4xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3: Configuración de modelos"
      ],
      "metadata": {
        "id": "6HhWefJxGgam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === MODELOS Y CONFIGURACIÓN ===\n",
        "model_name = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm_pipeline = pipeline(model=llm_model, tokenizer=tokenizer, task=\"text-generation\",\n",
        "                        do_sample=True, temperature=0.2, repetition_penalty=1.1,\n",
        "                        return_full_text=False, max_new_tokens=2048)\n",
        "\n",
        "pinecone_api_key = \"pcsk_6mZWLa_TF8H4JrD3FpzxXxBW91rkzYxKB66sv7YUFJ89nFdFDbV6gEVACuVNzSLdw2NJKW\"\n",
        "index_name = \"info-knowledge\"\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index_for_model(\n",
        "        name=index_name,\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\",\n",
        "        embed={\"model\": \"llama-text-embed-v2\", \"field_map\": {\"text\": \"chunk_text\"}, \"dimension\": 384}\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "x0JCym-rH5XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4: Herramientas (tools)"
      ],
      "metadata": {
        "id": "nZIoaRk5Gg_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === TOOLS ===\n",
        "def listar_pdfs_tool(directorio: str) -> dict:\n",
        "    \"\"\"\n",
        "    Lista todos los archivos PDF en un directorio dado y devuelve sus nombres relativos.\n",
        "    \"\"\"\n",
        "    archivos = []\n",
        "    for root, _, files in os.walk(directorio):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(\".pdf\"):\n",
        "                archivos.append(os.path.relpath(os.path.join(root, file), directorio))\n",
        "    return {\"archivos\": \", \".join(archivos)}\n",
        "\n",
        "def convertir_pdfs_tool(directorio: str, carpeta: str) -> dict:\n",
        "    \"\"\"\n",
        "    Convierte todos los archivos PDF en un directorio a archivos TXT y los guarda en una carpeta de salida.\n",
        "    \"\"\"\n",
        "    import pdfplumber\n",
        "\n",
        "    def extract_text_from_pdf(pdf_path):\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted = page.extract_text()\n",
        "                if extracted:\n",
        "                    text += extracted + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def process_large_pdf(pdf_path, input_folder, output_folder):\n",
        "        relative_path = os.path.relpath(pdf_path, input_folder)\n",
        "        txt_output_path = os.path.join(output_folder, os.path.splitext(relative_path)[0] + \".txt\")\n",
        "        os.makedirs(os.path.dirname(txt_output_path), exist_ok=True)\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        with open(txt_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        return txt_output_path\n",
        "\n",
        "    archivos_convertidos = []\n",
        "    for root, _, files in os.walk(directorio):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(\".pdf\"):\n",
        "                pdf_path = os.path.join(root, file)\n",
        "                txt_path = process_large_pdf(pdf_path, directorio, carpeta)\n",
        "                archivos_convertidos.append(txt_path)\n",
        "\n",
        "    return {\"conversion\": \"TXT generados: \" + ', '.join(archivos_convertidos)} if archivos_convertidos else {\"conversion\": \"No se generaron archivos.\"}"
      ],
      "metadata": {
        "id": "LXrSxlMYH8QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5: Indexación y recuperación"
      ],
      "metadata": {
        "id": "1960qRWHGhaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexar_tool(carpeta: str) -> dict:\n",
        "    \"\"\"\n",
        "    Indexa todos los archivos TXT en una carpeta usando embeddings y los sube a Pinecone.\n",
        "    \"\"\"\n",
        "    def extract_text_from_txt(txt_path):\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    RAW_KB = []\n",
        "    for root, _, files in os.walk(carpeta):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(\".txt\"):\n",
        "                path = os.path.join(root, file)\n",
        "                content = extract_text_from_txt(path)\n",
        "                RAW_KB.append(LangchainDocument(page_content=content, metadata={\"source\": path}))\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    split_docs = []\n",
        "    for doc in RAW_KB:\n",
        "        split_docs.extend(splitter.split_documents([doc]))\n",
        "\n",
        "    if not pc.has_index(index_name):\n",
        "        pc.create_index_for_model(\n",
        "            name=index_name,\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\",\n",
        "            embed={\"model\": \"llama-text-embed-v2\", \"field_map\": {\"text\": \"chunk_text\"}, \"dimension\": 384}\n",
        "        )\n",
        "\n",
        "    idx = pc.Index(index_name)\n",
        "    for doc in split_docs:\n",
        "        source = doc.metadata[\"source\"]\n",
        "        safe_id = unicodedata.normalize(\"NFKD\", source).encode(\"ascii\", \"ignore\").decode(\"ascii\").replace(os.sep, \"_\")\n",
        "        embedding = embedding_model.encode(doc.page_content).tolist()\n",
        "        metadata = doc.metadata.copy()\n",
        "        metadata[\"chunk_text\"] = doc.page_content\n",
        "        idx.upsert([(safe_id, embedding, metadata)])\n",
        "\n",
        "    return {\"indexacion\": f\"✅ {len(RAW_KB)} archivos, {len(split_docs)} chunks indexados en Pinecone.\"}\n",
        "\n",
        "def recuperar_contexto_tool(pregunta: str) -> dict:\n",
        "    \"\"\"\n",
        "    Recupera los documentos más relevantes para una pregunta usando Pinecone.\n",
        "    \"\"\"\n",
        "    embedding = embedding_model.encode(pregunta).tolist()\n",
        "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
        "    documentos = []\n",
        "    for match in result[\"matches\"]:\n",
        "        metadata = match.get(\"metadata\", {})\n",
        "        texto = metadata.get(\"chunk_text\", \"\") or metadata.get(\"text\", \"\")\n",
        "        if texto.strip():\n",
        "            documentos.append(LangchainDocument(page_content=texto, metadata=metadata))\n",
        "    return {\"documentos\": documentos, \"pregunta\": pregunta}\n"
      ],
      "metadata": {
        "id": "IqdzJQaaH81T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6: Generación de respuestas y subida a Moodle"
      ],
      "metadata": {
        "id": "jUmelKTPGh3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_respuesta_tool(pregunta: str, documentos: List[LangchainDocument]) -> dict:\n",
        "    \"\"\"\n",
        "    Genera 5 preguntas tipo test en formato AIKEN usando el contexto proporcionado.\n",
        "    \"\"\"\n",
        "    contexto = \"\\n\".join([doc.page_content for doc in documentos[:3]])\n",
        "    prompt = f\"\"\"Eres un experto en redacción de exámenes tipo test. Usa SOLO el contexto siguiente para generar 5 preguntas tipo test en formato AIKEN...\n",
        "\n",
        "Contexto:\n",
        "{contexto}\n",
        "\n",
        "Formato AIKEN sigue estrictamente el siguiente formato:\n",
        "Pregunta\n",
        "A) Opcion A\n",
        "B) Opcion B\n",
        "C) Opcion C\n",
        "D) Opcion D\n",
        "ANSWER: [Letra correcta]\n",
        "\n",
        "Genera las 5 preguntas a continuación:\n",
        "\"\"\"\n",
        "    output = llm_pipeline(prompt)[0][\"generated_text\"].strip()\n",
        "    with open(\"test_aiken.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(output)\n",
        "    return {\"respuesta\": output}\n",
        "\n",
        "def subir_aiken_a_moodle(nombre_archivo_txt, moodle_token, moodle_url_base):\n",
        "    \"\"\"\n",
        "    Sube un archivo AIKEN a Moodle usando su API.\n",
        "    \"\"\"\n",
        "    with open(nombre_archivo_txt, 'rb') as archivo:\n",
        "        response = requests.post(\n",
        "            f\"{moodle_url_base}/webservice/upload.php\",\n",
        "            params={\"token\": moodle_token},\n",
        "            files={\"file\": (nombre_archivo_txt, archivo, \"text/plain\")}\n",
        "        )\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        return {\"error\": f\"Error HTTP {response.status_code}\", \"detalle\": response.text}\n",
        "\n",
        "    try:\n",
        "        data = response.json()\n",
        "    except Exception as e:\n",
        "        return {\"error\": \"No se pudo decodificar JSON\", \"detalle\": str(e), \"respuesta_cruda\": response.text}\n",
        "\n",
        "    if not isinstance(data, list) or len(data) == 0 or \"itemid\" not in data[0]:\n",
        "        return {\"error\": \"Respuesta inesperada al subir archivo\", \"respuesta_completa\": data}\n",
        "\n",
        "    return {\"estado\": \" Archivo subido correctamente al draft de Moodle.\", \"itemid\": data[0][\"itemid\"]}\n",
        "\n",
        "\n",
        "def mover_a_privados_moodle(itemid_draft, moodle_token, moodle_url_base):\n",
        "    \"\"\"\n",
        "    Mueve un archivo subido como draft al área de archivos privados del usuario en Moodle.\n",
        "    \"\"\"\n",
        "    url = f\"{moodle_url_base}/webservice/rest/server.php\"\n",
        "    params = {\n",
        "        \"wstoken\": moodle_token,\n",
        "        \"wsfunction\": \"core_user_add_user_private_files\",\n",
        "        \"moodlewsrestformat\": \"json\",\n",
        "        \"draftid\": itemid_draft\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, params=params)\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            if response.headers.get(\"Content-Type\", \"\").startswith(\"application/json\"):\n",
        "                data = response.json()\n",
        "            else:\n",
        "                return {\n",
        "                    \"error\": \"Respuesta no es JSON\",\n",
        "                    \"status_code\": response.status_code,\n",
        "                    \"content_type\": response.headers.get(\"Content-Type\", \"\"),\n",
        "                    \"respuesta_cruda\": response.text\n",
        "                }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": \"Excepción al decodificar JSON\",\n",
        "                \"detalle\": str(e),\n",
        "                \"respuesta_cruda\": response.text\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"error\": \"No se pudo decodificar JSON al mover a privados\",\n",
        "            \"detalle\": str(e),\n",
        "            \"respuesta_cruda\": response.text\n",
        "        }\n",
        "\n",
        "    if isinstance(data, dict) and \"exception\" in data:\n",
        "        return {\"error\": data[\"errorcode\"], \"detalle\": data[\"message\"]}\n",
        "\n",
        "    return {\"estado\": \" Archivo movido correctamente a archivos privados.\"}"
      ],
      "metadata": {
        "id": "Px6GmvbEH9nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7: Grafo y aplicación"
      ],
      "metadata": {
        "id": "0U6IfXyAHJKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === GRAFO ===\n",
        "listar_pdfs_structured = StructuredTool.from_function(listar_pdfs_tool, args_schema=DirectorioInput)\n",
        "convertir_pdfs_structured = StructuredTool.from_function(convertir_pdfs_tool, args_schema=DirectorioCarpetaInput)\n",
        "indexar_structured = StructuredTool.from_function(indexar_tool, args_schema=CarpetaInput)\n",
        "recuperar_structured = StructuredTool.from_function(recuperar_contexto_tool, args_schema=PreguntaInput)\n",
        "generar_respuesta_structured = StructuredTool.from_function(generar_respuesta_tool, args_schema=PreguntaConDocsInput)\n",
        "\n",
        "grafo = StateGraph(WorkflowState)\n",
        "grafo.add_node(\"ListarPDF\", ToolNode.bind(listar_pdfs_structured))\n",
        "grafo.add_node(\"ConvertirPDF\", ToolNode.bind(convertir_pdfs_structured))\n",
        "grafo.add_node(\"Indexar\", ToolNode.bind(indexar_structured))\n",
        "grafo.add_node(\"Recuperar\", ToolNode.bind(recuperar_structured))\n",
        "grafo.add_node(\"Generar\", ToolNode.bind(generar_respuesta_structured))\n",
        "grafo.set_entry_point(\"ListarPDF\")\n",
        "grafo.add_edge(\"ListarPDF\", \"ConvertirPDF\")\n",
        "grafo.add_edge(\"ConvertirPDF\", \"Indexar\")\n",
        "grafo.add_edge(\"Indexar\", \"Recuperar\")\n",
        "grafo.add_edge(\"Recuperar\", \"Generar\")\n",
        "grafo.set_finish_point(\"Generar\")\n",
        "\n",
        "app = grafo.compile()\n"
      ],
      "metadata": {
        "id": "Tt3Ga8ftH-SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8: Funciones principales"
      ],
      "metadata": {
        "id": "-Guhs8uzHQfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === FUNCIONES PRINCIPALES ===\n",
        "def generar_aiken(directorio: str, carpeta: str, pregunta: str, nombre_archivo: str):\n",
        "    \"\"\"Genera preguntas AIKEN y las guarda en el archivo especificado.\"\"\"\n",
        "    estado = {\"directorio\": directorio, \"carpeta\": carpeta, \"pregunta\": pregunta}\n",
        "    result = app.invoke(estado)\n",
        "    with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(result.get(\"respuesta\", \"\"))\n",
        "    return result.get(\"respuesta\", \"Error al generar\")\n",
        "\n",
        "def subir_a_moodle(nombre_archivo: str, moodle_url: str, moodle_token: str):\n",
        "    \"\"\"Sube el archivo AIKEN especificado a Moodle.\"\"\"\n",
        "    try:\n",
        "        subir_result = subir_aiken_a_moodle(nombre_archivo, moodle_token, moodle_url)\n",
        "        if \"itemid\" not in subir_result:\n",
        "            return f\" Error al subir: {subir_result}\"\n",
        "        mover_result = mover_a_privados_moodle(subir_result[\"itemid\"], moodle_token, moodle_url)\n",
        "        return f\" Subido a Moodle!\\nDetalles: {mover_result}\"\n",
        "    except Exception as e:\n",
        "        return f\" Error crítico: {str(e)}\"\n",
        "\n",
        "def ejecutar_automatico(directorio: str, carpeta: str, pregunta: str, nombre_archivo: str,\n",
        "                        moodle_url: str, moodle_token: str) -> tuple[str, str]:\n",
        "    try:\n",
        "        preguntas = generar_aiken(directorio, carpeta, pregunta, nombre_archivo)\n",
        "        if not preguntas or \"Error\" in preguntas:\n",
        "            return preguntas or \" No se generaron preguntas\", \"\"\n",
        "        estado_moodle = subir_a_moodle(nombre_archivo, moodle_url, moodle_token)\n",
        "        return preguntas, estado_moodle\n",
        "    except Exception as e:\n",
        "        return f\" Error inesperado: {str(e)}\", \"\"\n"
      ],
      "metadata": {
        "id": "BXazovojH_s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9: Interfaz Gradio"
      ],
      "metadata": {
        "id": "hA4KVmhRHWNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === INTERFAZ GRADIO ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🤖 Agente RAG\")\n",
        "\n",
        "    with gr.Row():\n",
        "        directorio = gr.Textbox(label=\"* Carpeta con PDFs\")\n",
        "        carpeta_txt = gr.Textbox(label=\"* Carpeta de salida TXT\")\n",
        "        pregunta = gr.Textbox(label=\"* Tema para generar preguntas\")\n",
        "        nombre_archivo = gr.Textbox(label=\"* Nombre del archivo TXT\")\n",
        "\n",
        "    gr.Markdown(\"### Generar Preguntas\")\n",
        "    btn_generar = gr.Button(\" Generar Preguntas\")\n",
        "    salida_preguntas = gr.Textbox(label=\" Preguntas Generadas\", lines=10)\n",
        "\n",
        "    gr.Markdown(\"### + Subir a Moodle (Solo profesores)\")\n",
        "    with gr.Row():\n",
        "        moodle_url = gr.Textbox(label=\" URL de Moodle\")\n",
        "        moodle_token = gr.Textbox(label=\" Token API\")\n",
        "\n",
        "    with gr.Row():\n",
        "        btn_subir = gr.Button(\" Subir Archivo a Moodle\")\n",
        "        btn_automatico = gr.Button(\" Automático (Generar + Subir)\", variant=\"primary\")\n",
        "\n",
        "    salida_moodle = gr.Textbox(label=\" Estado de Moodle\", lines=3)\n",
        "\n",
        "    btn_generar.click(fn=generar_aiken,\n",
        "                     inputs=[directorio, carpeta_txt, pregunta, nombre_archivo],\n",
        "                     outputs=salida_preguntas)\n",
        "\n",
        "    btn_subir.click(fn=subir_a_moodle,\n",
        "                   inputs=[nombre_archivo, moodle_url, moodle_token],\n",
        "                   outputs=salida_moodle)\n",
        "\n",
        "    btn_automatico.click(fn=ejecutar_automatico,\n",
        "                         inputs=[directorio, carpeta_txt, pregunta, nombre_archivo, moodle_url, moodle_token],\n",
        "                         outputs=[salida_preguntas, salida_moodle])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "LJtu7EyRIATm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}